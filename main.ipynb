{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6bde3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install langchain langgraph faiss-cpu wikipedia langsmith sentence-transformers\n",
    "\n",
    "# Imports\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableMap\n",
    "from langchain.tools import WikipediaQueryRun\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langgraph.graph import StateGraph\n",
    "from langchain_core.tracers.langchain import LangChainTracer\n",
    "\n",
    "import wikipedia\n",
    "import wikipediaapi\n",
    "import requests\n",
    "from bs4 import BeautifulSoup \n",
    "import os\n",
    "import ast\n",
    "import re\n",
    "import time\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4696ba14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangSmith setup\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"mcq_solver\"\n",
    "\n",
    "tracer = LangChainTracer(project_name=\"mcq_solver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1466ebee",
   "metadata": {},
   "outputs": [],
   "source": [
    "aval_llm = lambda: ChatOpenAI(\n",
    "    model=\"meta.llama3-1-8b-instruct-v1:0\",\n",
    "    api_key=\"\",\n",
    "    base_url=\"\"\n",
    ")\n",
    "\n",
    "llm_first_guess = aval_llm()\n",
    "llm_rephraser = aval_llm()\n",
    "llm_comparator = aval_llm()\n",
    "llm_expander = aval_llm()\n",
    "llm_decomposer = aval_llm()\n",
    "\n",
    "# Wikipedia tool\n",
    "wiki_tool = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "\n",
    "# Embeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa72e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_wiki = wikipediaapi.Wikipedia(\n",
    "    language='en',\n",
    "    user_agent=''\n",
    ")\n",
    "\n",
    "embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "# splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "def _get_full_text_for_title(title: str) -> str:\n",
    "    \"\"\"Try wikipediaapi first; fall back to simple HTML scraping if needed.\"\"\"\n",
    "    try:\n",
    "        page = wiki_wiki.page(title)\n",
    "        if page.exists() and page.text and page.text.strip():\n",
    "            return page.text\n",
    "    except Exception:\n",
    "        # continue to scraping fallback\n",
    "        pass\n",
    "\n",
    "    # Fallback: scrape the article paragraphs (respectful User-Agent)\n",
    "    url = f\"https://en.wikipedia.org/wiki/{title.replace(' ', '_')}\"\n",
    "    try:\n",
    "        r = requests.get(url, headers={'User-Agent': ''}, timeout=8)\n",
    "        if r.status_code == 200:\n",
    "            soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "            paras = [p.get_text().strip() for p in soup.select(\"p\") if p.get_text().strip()]\n",
    "            if paras:\n",
    "                return \"\\n\".join(paras)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def get_wikipedia_page(title):\n",
    "    \"\"\"Get full text of a Wikipedia page using wikipedia-api\"\"\"\n",
    "    page = wiki_wiki.page(title)\n",
    "    if not page.exists():\n",
    "        return \"\"\n",
    "    return page.text\n",
    "\n",
    "\n",
    "def scrape_wikipedia(title):\n",
    "    \"\"\"Scrape Wikipedia page with BeautifulSoup as fallback\"\"\"\n",
    "    url = f\"https://en.wikipedia.org/wiki/{title.replace(' ', '_')}\"\n",
    "    r = requests.get(url)\n",
    "    if r.status_code != 200:\n",
    "        return \"\"\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    paragraphs = [p.text for p in soup.select(\"p\")]\n",
    "    return \"\\n\".join(paragraphs)\n",
    "\n",
    "\n",
    "def search_wikipedia(query, max_pages=3):\n",
    "    \"\"\"Search Wikipedia and return full text from top results\"\"\"\n",
    "    try:\n",
    "        titles = wikipedia.search(query, results=max_pages)\n",
    "        texts = []\n",
    "        for t in titles:\n",
    "            text = get_wikipedia_page(t)\n",
    "            if not text:\n",
    "                text = scrape_wikipedia(t)  # fallback\n",
    "            texts.append(text)\n",
    "        return \"\\n\".join(texts)\n",
    "    except:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33d88f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_wiki_search(queries, max_titles_per_query: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    For each query string in `queries`:\n",
    "      - search wikipedia (top N titles),\n",
    "      - fetch full page text for each title (api -> fallback scrape),\n",
    "      - gather texts (deduplicated) and return a single combined string.\n",
    "\n",
    "    This returns ONE big text blob (string). Your existing\n",
    "    `retrieve_top_chunks(text, query)` will then split/index it and\n",
    "    return the top snippets for the rephrased query — exactly like your\n",
    "    current pipeline expects.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    seen_titles = set()\n",
    "\n",
    "    for q in queries:\n",
    "        if not q or not q.strip():\n",
    "            continue\n",
    "\n",
    "        # Use python-wikipedia search to get candidate titles\n",
    "        try:\n",
    "            titles = wikipedia.search(q, results=max_titles_per_query)\n",
    "        except Exception:\n",
    "            titles = []\n",
    "\n",
    "        for t in titles:\n",
    "            if t in seen_titles:\n",
    "                continue\n",
    "            seen_titles.add(t)\n",
    "            txt = _get_full_text_for_title(t)\n",
    "            if txt and len(txt.strip()) > 0:\n",
    "                results.append(txt)\n",
    "\n",
    "\n",
    "    if not results and 'wiki_tool' in globals():\n",
    "        for q in queries:\n",
    "            try:\n",
    "                r = wiki_tool.run({\"query\": q.strip()})\n",
    "                if r and r.strip():\n",
    "                    results.append(r)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    return \"\\n\\n\".join(results)\n",
    "\n",
    "\n",
    "def build_hybrid_retriever(text):\n",
    "    text_splitter = SentenceTransformersTokenTextSplitter(chunk_size=1, chunk_overlap=0)\n",
    "    sentences = text_splitter.split_text(text)\n",
    "    bm25 = BM25Retriever.from_texts(sentences)\n",
    "    faiss_store = FAISS.from_texts(sentences, embedding_model) \n",
    "    faiss = faiss_store.as_retriever()\n",
    "    return EnsembleRetriever(retrievers=[bm25, faiss], weights=[0.5, 0.5])\n",
    "\n",
    "def retrieve_top_chunks(text, query, k=3):\n",
    "    retriever = build_hybrid_retriever(text)\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    return [doc.page_content for doc in docs[:k]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a58b232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def multi_wiki_search(queries, question, k=5):\n",
    "#     \"\"\"Build a FAISS retriever over Wikipedia search results and return top-k chunks\"\"\"\n",
    "#     combined_text = \"\"\n",
    "#     for q in queries:\n",
    "#         combined_text += search_wikipedia(q) + \"\\n\"\n",
    "\n",
    "#     # Split into chunks\n",
    "#     chunks = splitter.split_text(combined_text)\n",
    "\n",
    "#     if not chunks:\n",
    "#         return \"\"\n",
    "\n",
    "#     # Build FAISS index\n",
    "#     db = FAISS.from_texts(chunks, embedder)\n",
    "#     retriever = db.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "#     # Retrieve most relevant chunks for the actual question\n",
    "#     docs = retriever.get_relevant_documents(question)\n",
    "#     return \"\\n\".join([d.page_content for d in docs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0f9ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rephrase\n",
    "\n",
    "first_guess_chain = (\n",
    "    PromptTemplate.from_template(\"\"\"\n",
    "                                 Answer the following multiple choice question:\n",
    "                                 \n",
    "                                 Question:\n",
    "                                 {question}\n",
    "                                 \n",
    "                                 Choices:\n",
    "                                 {choices}\n",
    "                                 \n",
    "                                 only return the letter of the correct answer since it might cause conflict.\n",
    "                                 Answer:\n",
    "                                 \"\"\")\n",
    "    | llm_first_guess\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rephrase_chain = (\n",
    "    PromptTemplate.from_template(\"\"\"\n",
    "                                 Rephrase the following question into a single, concise Wikipedia search query. Avoid explanations. Just return the query.\n",
    "                                 if the question is already a search query, just return it.\n",
    "                                 \n",
    "                                 Question:\n",
    "                                {question}\n",
    "                                \n",
    "                                rephrased question:\n",
    "                                \"\"\")\n",
    "    | llm_rephraser\n",
    "    | StrOutputParser()\n",
    ").with_config(tags=[\"rephrase\"], run_name=\"rephrase_chain\", callbacks=[tracer])\n",
    "\n",
    "# Expand\n",
    "expand_chain = (\n",
    "    PromptTemplate.from_template(\"\"\"\n",
    "                                 Extract the keywords containing technical terms and synonyms from the question and return zero to two search queries that are easy to search for on Wikipedia in order to find the answer.\n",
    "                                 Try your best not to give too similar querries.\n",
    "                                 \n",
    "                                 You will also be given the potential answer to the question. analyze the answer, if you find the answer incorrect ignore it, otherwise write a query that can support your opinion by searching that query on Wikipedia.\n",
    "                                 \n",
    "                                 only return the queries in the format of a python list of strings.\n",
    "                                 do not return anything else since it might cause conflict.\n",
    "                                 important: if you cannot write any querry that improves the given query, just return \"[]\"\n",
    "                                 do not return the exact given question.\n",
    "                                 example:\n",
    "                                 [\"search query 1\", \"search query 2\"]\n",
    "                                 example:\n",
    "                                 []\n",
    "                                 \n",
    "                                 Question:\n",
    "                                 {question}\n",
    "                                 \n",
    "                                 Potential answer:\n",
    "                                 {answer}\n",
    "                                 \n",
    "                                 queries:\n",
    "                                \"\"\")\n",
    "    | llm_expander\n",
    "    | StrOutputParser()\n",
    ").with_config(tags=[\"expand\"], run_name=\"expand_chain\", callbacks=[tracer])\n",
    "\n",
    "# Decompose\n",
    "# decompose_chain = (\n",
    "#     PromptTemplate.from_template(\"Break this question into 2–3 simpler subquestions:\\n\\n{question}\")\n",
    "#     | llm_decomposer\n",
    "#     | StrOutputParser()\n",
    "# ).with_config(tags=[\"decompose\"], run_name=\"decompose_chain\", callbacks=[tracer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c889924e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Hybrid Retrieval can also be used but the current flow seems to work better!\n",
    "\n",
    "\n",
    "# def multi_wiki_search(queries):\n",
    "#     results = []\n",
    "#     for q in queries:\n",
    "#         try:\n",
    "#             results.append(wiki_tool.run({\"query\": q.strip()}))\n",
    "#         except:\n",
    "#             continue\n",
    "#     return \"\\n\\n\".join(results)\n",
    "\n",
    "# # Hybrid retrieval\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "# def build_hybrid_retriever(text):\n",
    "#     chunks = text_splitter.split_text(text)\n",
    "#     bm25 = BM25Retriever.from_texts(chunks)\n",
    "#     faiss_store = FAISS.from_texts(chunks, embedding_model)\n",
    "#     faiss = faiss_store.as_retriever()\n",
    "#     return EnsembleRetriever(retrievers=[bm25, faiss], weights=[0.5, 0.5])\n",
    "\n",
    "# def retrieve_top_chunks(text, query, k=3):\n",
    "#     retriever = build_hybrid_retriever(text)\n",
    "#     docs = retriever.get_relevant_documents(query)\n",
    "#     return [doc.page_content for doc in docs[:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2b04cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_chain = (\n",
    "    RunnableMap({\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"context\": lambda x: \"\\n\\n\".join(x[\"snippets\"]),\n",
    "        \"A\": lambda x: x[\"choices\"][0],\n",
    "        \"B\": lambda x: x[\"choices\"][1],\n",
    "        \"C\": lambda x: x[\"choices\"][2],\n",
    "        \"D\": lambda x: x[\"choices\"][3],\n",
    "        \"E\": lambda x: x[\"choices\"][4],\n",
    "    })\n",
    "    | PromptTemplate.from_template(\n",
    "        \"\"\"You are a multiple-choice question solver.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Choices:\n",
    "A. {A}\n",
    "B. {B}\n",
    "C. {C}\n",
    "D. {D}\n",
    "E. {E}\n",
    "\n",
    "Wikipedia context:\n",
    "{context}\n",
    "\n",
    "Which choice is best supported? Respond with one letter: A, B, C, D, or E.\n",
    "Return only the letter, no other text.\"\"\"\n",
    "    )\n",
    "    | llm_comparator\n",
    "    | StrOutputParser()\n",
    ").with_config(tags=[\"compare\"], run_name=\"compare_chain\", callbacks=[tracer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982ed961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def retrieve_evidence_per_option(question, option, retriever, k=3):\n",
    "#     query = f\"{question} {option}\"\n",
    "#     docs = retriever.get_relevant_documents(query)\n",
    "#     return \"\\n\".join([d.page_content for d in docs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230279af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_evidence_for_option(question, option, retriever, k=3):\n",
    "    \"\"\"Retrieve supporting Wikipedia chunks for a specific option\"\"\"\n",
    "    query = f\"{question} {option}\"\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    return \"\\n\".join([d.page_content for d in docs[:k]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e536f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge_option(llm, question, option, evidence):\n",
    "    \"\"\"Ask the LLM if evidence supports the option\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "Question: {question}\n",
    "\n",
    "Option: {option}\n",
    "\n",
    "Evidence from Wikipedia:\n",
    "{evidence}\n",
    "\n",
    "Does the evidence support that this option is correct?\n",
    "Answer strictly with one of: YES, NO, or UNKNOWN.\n",
    "\"\"\"\n",
    "    result = llm.invoke(prompt).content.strip().upper()\n",
    "    if \"YES\" in result:\n",
    "        return \"YES\"\n",
    "    elif \"NO\" in result:\n",
    "        return \"NO\"\n",
    "    else:\n",
    "        return \"UNKNOWN\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e1861b",
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_to_number = {\"A\":0, \"B\":1, \"C\":2, \"D\":3, \"E\":4}\n",
    "def answer_mcq_graph(question, choices):\n",
    "    \"\"\"\n",
    "    Multiple Choice QA with Wikipedia + Hybrid Retrieval.\n",
    "    \n",
    "    Steps:\n",
    "    1. Use queries (from rephrase/expand/decompose chains).\n",
    "    2. Retrieve Wikipedia text for all queries (multi_wiki_search).\n",
    "    3. Split + hybrid index, then retrieve top chunks for the rephrased query.\n",
    "    4. Compare each option against the retrieved evidence.\n",
    "    5. Return the best option + detailed scores.\n",
    "    \"\"\"\n",
    "     \n",
    "    print(f\"question: {question}, \\n chouces: {choices} \\n ----------------\", )\n",
    "    \n",
    "    answer = first_guess_chain.invoke({\"question\": question, \"choices\": choices})\n",
    "    print(f\"answer: {answer} \\n ---------------\")\n",
    "    \n",
    "    rephrased = rephrase_chain.invoke({\"question\": question})\n",
    "    print(f\"rephrased: {rephrased} \\n ---------------\")\n",
    "    \n",
    "    letter_to_number = {\"A\":0, \"B\":1, \"C\":2, \"D\":3, \"E\":4}\n",
    "    expanded = expand_chain.invoke({\"answer\": choices[letter_to_number[re.search(r'\\b[A-E]\\b', answer).group()]], \"question\": question})\n",
    "    print(f\"expanded: {expanded} \\n ---------------\")\n",
    "    # decomposed = decompose_chain.invoke({\"question\": question})\n",
    "    # print(f\"decomposed: {decomposed} \\n ---------------\")\n",
    "    \n",
    "    clean_expanded = ''\n",
    "    is_brace_open = False\n",
    "    for i in expanded:\n",
    "        if i == '[' or is_brace_open:\n",
    "            clean_expanded += i \n",
    "            is_brace_open = True\n",
    "        if i == ']':\n",
    "            break\n",
    "    print(f\"expanded: {clean_expanded} \\n ---------------\")\n",
    "    all_queries = [rephrased.replace('\\n', '')] + ast.literal_eval(clean_expanded)\n",
    "    if all_queries[0] == all_queries[1]:\n",
    "        all_queries = all_queries[1:]\n",
    "    print(f\"all queries: {all_queries} \\n -------------------------\")\n",
    "\n",
    "    # 1. Gather Wikipedia text from all queries\n",
    "    wiki_text = multi_wiki_search(all_queries)\n",
    "    if not wiki_text.strip():\n",
    "        return None, {opt: \"NO EVIDENCE\" for opt in choices}\n",
    "\n",
    "    # 2. Retrieve top chunks using hybrid retriever\n",
    "    top_chunks = retrieve_top_chunks(wiki_text, rephrased, k=5)\n",
    "    \n",
    "    for chunk in top_chunks:\n",
    "        print(f\"chunk: {chunk}\")\n",
    "    print(\"--------------------------------\")\n",
    "    # 3. Ask comparator LLM to judge each option against evidence\n",
    "    result = compare_chain.invoke({\n",
    "        \"question\": question,\n",
    "        \"choices\": choices,\n",
    "        \"snippets\": top_chunks\n",
    "    })\n",
    "    \n",
    "    print(f\"result: {result} \\n -------------------------\")\n",
    "    # 4. Parse answer\n",
    "    best_answer = result.strip().upper()\n",
    "    # Remove punctuation and extra words\n",
    "    best_answer = best_answer.replace(\".\", \"\").replace(\"ANSWER:\", \"\").strip()\n",
    "    # Keep only first valid letter\n",
    "    if best_answer and best_answer[0] in \"ABCDE\":\n",
    "        best_answer = best_answer[0]\n",
    "    else:\n",
    "        return None, {c: \"UNKNOWN\" for c in choices}\n",
    "\n",
    "    # 5. Map back to choices\n",
    "    scores = {c: (\"YES\" if i == \"ABCDE\".index(best_answer) else \"NO\")\n",
    "              for i, c in enumerate(choices)}\n",
    "\n",
    "    return choices[\"ABCDE\".index(best_answer)], scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4634c94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the main sequence in astronomy?\"\n",
    "choices = [\n",
    "    \"A group of galaxies\",\n",
    "    \"A type of telescope\",\n",
    "    \"A phase in stellar evolution\",\n",
    "    \"A planetary orbit\",\n",
    "    \"A black hole classification\"\n",
    "]\n",
    "\n",
    "answer = answer_mcq_graph(question, choices)\n",
    "print(\"Predicted answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5c3fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = pd.read_csv(\"data/train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3467668",
   "metadata": {},
   "outputs": [],
   "source": [
    "Choices = []\n",
    "Questions = []\n",
    "Answers = []\n",
    "for index, row in train_file.iterrows():\n",
    "    Choices.append([row['A'], row['B'], row['C'], row['D'], row['E']])\n",
    "    Questions.append(row['prompt'])\n",
    "    Answers.append(row['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752f8e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd9ed36",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (len(predicted),len(Questions)):\n",
    "    predict = answer_mcq_graph(Questions[i], Choices[i])\n",
    "    if predict != Answers[i]:\n",
    "        predicted.append(re.search(r'\\b[A-E]\\b', predict).group())\n",
    "        print(\"FINAL ANSWER:\")\n",
    "        print(predict, i)\n",
    "    else:\n",
    "        predicted.append(\"_\")\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db252764",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file['predicted'] = predicted\n",
    "train_file.to_csv(\"Predicted_by_AgenticAI.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
