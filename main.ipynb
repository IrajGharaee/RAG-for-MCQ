{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b6bde3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# !pip install langchain langgraph faiss-cpu wikipedia langsmith sentence-transformers\n",
    "\n",
    "# Imports\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableMap\n",
    "from langchain.tools import WikipediaQueryRun\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langgraph.graph import StateGraph\n",
    "from langchain_core.tracers.langchain import LangChainTracer\n",
    "\n",
    "import wikipedia\n",
    "import wikipediaapi\n",
    "import requests\n",
    "from bs4 import BeautifulSoup  # pyright: ignore[reportMissingModuleSource]\n",
    "import os\n",
    "import ast\n",
    "import re\n",
    "import time\n",
    "import pandas as pd  # pyright: ignore[reportMissingImports]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4696ba14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to get info from https://api.smith.langchain.com: LangSmithError('Failed to GET /info in LangSmith API. HTTPError(\\'403 Client Error: Forbidden for url: https://api.smith.langchain.com/info\\', \\'\\\\n<html><head>\\\\n<meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8\">\\\\n<title>403 Forbidden</title>\\\\n</head>\\\\n<body text=#000000 bgcolor=#ffffff>\\\\n<h1>Error: Forbidden</h1>\\\\n<h2>Your client does not have permission to get URL <code>/info</code> from this server.</h2>\\\\n<h2></h2>\\\\n</body></html>\\\\n\\')')\n",
      "Failed to batch ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch', '\\n<html><head>\\n<meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8\">\\n<title>403 Forbidden</title>\\n</head>\\n<body text=#000000 bgcolor=#ffffff>\\n<h1>Error: Forbidden</h1>\\n<h2>Your client does not have permission to get URL <code>/runs/batch</code> from this server.</h2>\\n<h2></h2>\\n</body></html>\\n')\n",
      "Failed to batch ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch', '\\n<html><head>\\n<meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8\">\\n<title>403 Forbidden</title>\\n</head>\\n<body text=#000000 bgcolor=#ffffff>\\n<h1>Error: Forbidden</h1>\\n<h2>Your client does not have permission to get URL <code>/runs/batch</code> from this server.</h2>\\n<h2></h2>\\n</body></html>\\n')\n",
      "Failed to batch ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch', '\\n<html><head>\\n<meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8\">\\n<title>403 Forbidden</title>\\n</head>\\n<body text=#000000 bgcolor=#ffffff>\\n<h1>Error: Forbidden</h1>\\n<h2>Your client does not have permission to get URL <code>/runs/batch</code> from this server.</h2>\\n<h2></h2>\\n</body></html>\\n')\n"
     ]
    }
   ],
   "source": [
    "# LangSmith setup\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"mcq_solver\"\n",
    "\n",
    "tracer = LangChainTracer(project_name=\"mcq_solver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1466ebee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IrajGharaee\\AppData\\Local\\Temp\\ipykernel_7568\\995700128.py:2: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  aval_llm = lambda: ChatOpenAI(\n",
      "C:\\Users\\IrajGharaee\\AppData\\Local\\Temp\\ipykernel_7568\\995700128.py:18: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "# LLaMA 3 via Aval AI\n",
    "aval_llm = lambda: ChatOpenAI(\n",
    "    model=\"meta.llama3-1-8b-instruct-v1:0\",\n",
    "    api_key=\"\",\n",
    "    base_url=\"\"\n",
    ")\n",
    "\n",
    "llm_first_guess = aval_llm()\n",
    "llm_rephraser = aval_llm()\n",
    "llm_comparator = aval_llm()\n",
    "llm_expander = aval_llm()\n",
    "llm_decomposer = aval_llm()\n",
    "\n",
    "# Wikipedia tool\n",
    "wiki_tool = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "\n",
    "# Embeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa72e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_wiki = wikipediaapi.Wikipedia(\n",
    "    language='en',\n",
    "    user_agent=''\n",
    ")\n",
    "\n",
    "embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "# splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "def _get_full_text_for_title(title: str) -> str:\n",
    "    \"\"\"Try wikipediaapi first; fall back to simple HTML scraping if needed.\"\"\"\n",
    "    try:\n",
    "        page = wiki_wiki.page(title)\n",
    "        if page.exists() and page.text and page.text.strip():\n",
    "            return page.text\n",
    "    except Exception:\n",
    "        # continue to scraping fallback\n",
    "        pass\n",
    "\n",
    "    # Fallback: scrape the article paragraphs (respectful User-Agent)\n",
    "    url = f\"https://en.wikipedia.org/wiki/{title.replace(' ', '_')}\"\n",
    "    try:\n",
    "        r = requests.get(url, headers={'User-Agent': ''}, timeout=8)\n",
    "        if r.status_code == 200:\n",
    "            soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "            paras = [p.get_text().strip() for p in soup.select(\"p\") if p.get_text().strip()]\n",
    "            if paras:\n",
    "                return \"\\n\".join(paras)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def get_wikipedia_page(title):\n",
    "    \"\"\"Get full text of a Wikipedia page using wikipedia-api\"\"\"\n",
    "    page = wiki_wiki.page(title)\n",
    "    if not page.exists():\n",
    "        return \"\"\n",
    "    return page.text\n",
    "\n",
    "\n",
    "def scrape_wikipedia(title):\n",
    "    \"\"\"Scrape Wikipedia page with BeautifulSoup as fallback\"\"\"\n",
    "    url = f\"https://en.wikipedia.org/wiki/{title.replace(' ', '_')}\"\n",
    "    r = requests.get(url)\n",
    "    if r.status_code != 200:\n",
    "        return \"\"\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    paragraphs = [p.text for p in soup.select(\"p\")]\n",
    "    return \"\\n\".join(paragraphs)\n",
    "\n",
    "\n",
    "def search_wikipedia(query, max_pages=3):\n",
    "    \"\"\"Search Wikipedia and return full text from top results\"\"\"\n",
    "    try:\n",
    "        titles = wikipedia.search(query, results=max_pages)\n",
    "        texts = []\n",
    "        for t in titles:\n",
    "            text = get_wikipedia_page(t)\n",
    "            if not text:\n",
    "                text = scrape_wikipedia(t)  # fallback\n",
    "            texts.append(text)\n",
    "        return \"\\n\".join(texts)\n",
    "    except:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d33d88f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_wiki_search(queries, max_titles_per_query: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    For each query string in `queries`:\n",
    "      - search wikipedia (top N titles),\n",
    "      - fetch full page text for each title (api -> fallback scrape),\n",
    "      - gather texts (deduplicated) and return a single combined string.\n",
    "\n",
    "    This returns ONE big text blob (string). Your existing\n",
    "    `retrieve_top_chunks(text, query)` will then split/index it and\n",
    "    return the top snippets for the rephrased query — exactly like your\n",
    "    current pipeline expects.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    seen_titles = set()\n",
    "\n",
    "    for q in queries:\n",
    "        if not q or not q.strip():\n",
    "            continue\n",
    "\n",
    "        # Use python-wikipedia search to get candidate titles\n",
    "        try:\n",
    "            titles = wikipedia.search(q, results=max_titles_per_query)\n",
    "        except Exception:\n",
    "            titles = []\n",
    "\n",
    "        for t in titles:\n",
    "            if t in seen_titles:\n",
    "                continue\n",
    "            seen_titles.add(t)\n",
    "            txt = _get_full_text_for_title(t)\n",
    "            if txt and len(txt.strip()) > 0:\n",
    "                results.append(txt)\n",
    "\n",
    "    # As a final fallback (rare), fall back to your existing wiki tool if it exists\n",
    "    # (keeps backward compatibility). `wiki_tool` was present in your notebook.\n",
    "    if not results and 'wiki_tool' in globals():\n",
    "        for q in queries:\n",
    "            try:\n",
    "                r = wiki_tool.run({\"query\": q.strip()})\n",
    "                if r and r.strip():\n",
    "                    results.append(r)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    # Return combined text (what retrieve_top_chunks expects)\n",
    "    return \"\\n\\n\".join(results)\n",
    "\n",
    "\n",
    "# Hybrid retrieval helpers (left compatible with your current pipeline)\n",
    "# If you already had build_hybrid_retriever / retrieve_top_chunks in the notebook\n",
    "# replace them with these or keep yours — function names and behavior are identical.\n",
    "\n",
    "def build_hybrid_retriever(text):\n",
    "    text_splitter = SentenceTransformersTokenTextSplitter(chunk_size=1, chunk_overlap=0)\n",
    "    sentences = text_splitter.split_text(text)\n",
    "    bm25 = BM25Retriever.from_texts(sentences)\n",
    "    faiss_store = FAISS.from_texts(sentences, embedding_model)  # uses your embedding_model\n",
    "    faiss = faiss_store.as_retriever()\n",
    "    return EnsembleRetriever(retrievers=[bm25, faiss], weights=[0.5, 0.5])\n",
    "\n",
    "def retrieve_top_chunks(text, query, k=3):\n",
    "    retriever = build_hybrid_retriever(text)\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    return [doc.page_content for doc in docs[:k]]\n",
    "# --- end replacement cell ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a58b232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def multi_wiki_search(queries, question, k=5):\n",
    "#     \"\"\"Build a FAISS retriever over Wikipedia search results and return top-k chunks\"\"\"\n",
    "#     combined_text = \"\"\n",
    "#     for q in queries:\n",
    "#         combined_text += search_wikipedia(q) + \"\\n\"\n",
    "\n",
    "#     # Split into chunks\n",
    "#     chunks = splitter.split_text(combined_text)\n",
    "\n",
    "#     if not chunks:\n",
    "#         return \"\"\n",
    "\n",
    "#     # Build FAISS index\n",
    "#     db = FAISS.from_texts(chunks, embedder)\n",
    "#     retriever = db.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "#     # Retrieve most relevant chunks for the actual question\n",
    "#     docs = retriever.get_relevant_documents(question)\n",
    "#     return \"\\n\".join([d.page_content for d in docs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e0f9ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rephrase\n",
    "\n",
    "first_guess_chain = (\n",
    "    PromptTemplate.from_template(\"\"\"\n",
    "                                 Answer the following multiple choice question:\n",
    "                                 \n",
    "                                 Question:\n",
    "                                 {question}\n",
    "                                 \n",
    "                                 Choices:\n",
    "                                 {choices}\n",
    "                                 \n",
    "                                 only return the letter of the correct answer since it might cause conflict.\n",
    "                                 Answer:\n",
    "                                 \"\"\")\n",
    "    | llm_first_guess\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rephrase_chain = (\n",
    "    PromptTemplate.from_template(\"\"\"\n",
    "                                 Rephrase the following question into a single, concise Wikipedia search query. Avoid explanations. Just return the query.\n",
    "                                 if the question is already a search query, just return it.\n",
    "                                 \n",
    "                                 Question:\n",
    "                                {question}\n",
    "                                \n",
    "                                rephrased question:\n",
    "                                \"\"\")\n",
    "    | llm_rephraser\n",
    "    | StrOutputParser()\n",
    ").with_config(tags=[\"rephrase\"], run_name=\"rephrase_chain\", callbacks=[tracer])\n",
    "\n",
    "# Expand\n",
    "expand_chain = (\n",
    "    PromptTemplate.from_template(\"\"\"\n",
    "                                 Extract the keywords containing technical terms and synonyms from the question and return zero to two search queries that are easy to search for on Wikipedia in order to find the answer.\n",
    "                                 Try your best not to give too similar querries.\n",
    "                                 \n",
    "                                 You will also be given the potential answer to the question. analyze the answer, if you find the answer incorrect ignore it, otherwise write a query that can support your opinion by searching that query on Wikipedia.\n",
    "                                 \n",
    "                                 only return the queries in the format of a python list of strings.\n",
    "                                 do not return anything else since it might cause conflict.\n",
    "                                 important: if you cannot write any querry that improves the given query, just return \"[]\"\n",
    "                                 do not return the exact given question.\n",
    "                                 example:\n",
    "                                 [\"search query 1\", \"search query 2\"]\n",
    "                                 example:\n",
    "                                 []\n",
    "                                 \n",
    "                                 Question:\n",
    "                                 {question}\n",
    "                                 \n",
    "                                 Potential answer:\n",
    "                                 {answer}\n",
    "                                 \n",
    "                                 queries:\n",
    "                                \"\"\")\n",
    "    | llm_expander\n",
    "    | StrOutputParser()\n",
    ").with_config(tags=[\"expand\"], run_name=\"expand_chain\", callbacks=[tracer])\n",
    "\n",
    "# Decompose\n",
    "# decompose_chain = (\n",
    "#     PromptTemplate.from_template(\"Break this question into 2–3 simpler subquestions:\\n\\n{question}\")\n",
    "#     | llm_decomposer\n",
    "#     | StrOutputParser()\n",
    "# ).with_config(tags=[\"decompose\"], run_name=\"decompose_chain\", callbacks=[tracer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c889924e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Hybrid Retrieval can also be used but the current flow seems to work better!\n",
    "\n",
    "\n",
    "# def multi_wiki_search(queries):\n",
    "#     results = []\n",
    "#     for q in queries:\n",
    "#         try:\n",
    "#             results.append(wiki_tool.run({\"query\": q.strip()}))\n",
    "#         except:\n",
    "#             continue\n",
    "#     return \"\\n\\n\".join(results)\n",
    "\n",
    "# # Hybrid retrieval\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "# def build_hybrid_retriever(text):\n",
    "#     chunks = text_splitter.split_text(text)\n",
    "#     bm25 = BM25Retriever.from_texts(chunks)\n",
    "#     faiss_store = FAISS.from_texts(chunks, embedding_model)\n",
    "#     faiss = faiss_store.as_retriever()\n",
    "#     return EnsembleRetriever(retrievers=[bm25, faiss], weights=[0.5, 0.5])\n",
    "\n",
    "# def retrieve_top_chunks(text, query, k=3):\n",
    "#     retriever = build_hybrid_retriever(text)\n",
    "#     docs = retriever.get_relevant_documents(query)\n",
    "#     return [doc.page_content for doc in docs[:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d2b04cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_chain = (\n",
    "    RunnableMap({\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"context\": lambda x: \"\\n\\n\".join(x[\"snippets\"]),\n",
    "        \"A\": lambda x: x[\"choices\"][0],\n",
    "        \"B\": lambda x: x[\"choices\"][1],\n",
    "        \"C\": lambda x: x[\"choices\"][2],\n",
    "        \"D\": lambda x: x[\"choices\"][3],\n",
    "        \"E\": lambda x: x[\"choices\"][4],\n",
    "    })\n",
    "    | PromptTemplate.from_template(\n",
    "        \"\"\"You are a multiple-choice question solver.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Choices:\n",
    "A. {A}\n",
    "B. {B}\n",
    "C. {C}\n",
    "D. {D}\n",
    "E. {E}\n",
    "\n",
    "Wikipedia context:\n",
    "{context}\n",
    "\n",
    "Which choice is best supported? Respond with one letter: A, B, C, D, or E.\n",
    "Return only the letter, no other text.\"\"\"\n",
    "    )\n",
    "    | llm_comparator\n",
    "    | StrOutputParser()\n",
    ").with_config(tags=[\"compare\"], run_name=\"compare_chain\", callbacks=[tracer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "982ed961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def retrieve_evidence_per_option(question, option, retriever, k=3):\n",
    "#     query = f\"{question} {option}\"\n",
    "#     docs = retriever.get_relevant_documents(query)\n",
    "#     return \"\\n\".join([d.page_content for d in docs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "230279af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_evidence_for_option(question, option, retriever, k=3):\n",
    "    \"\"\"Retrieve supporting Wikipedia chunks for a specific option\"\"\"\n",
    "    query = f\"{question} {option}\"\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    return \"\\n\".join([d.page_content for d in docs[:k]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52e536f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge_option(llm, question, option, evidence):\n",
    "    \"\"\"Ask the LLM if evidence supports the option\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "Question: {question}\n",
    "\n",
    "Option: {option}\n",
    "\n",
    "Evidence from Wikipedia:\n",
    "{evidence}\n",
    "\n",
    "Does the evidence support that this option is correct?\n",
    "Answer strictly with one of: YES, NO, or UNKNOWN.\n",
    "\"\"\"\n",
    "    result = llm.invoke(prompt).content.strip().upper()\n",
    "    if \"YES\" in result:\n",
    "        return \"YES\"\n",
    "    elif \"NO\" in result:\n",
    "        return \"NO\"\n",
    "    else:\n",
    "        return \"UNKNOWN\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19e1861b",
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_to_number = {\"A\":0, \"B\":1, \"C\":2, \"D\":3, \"E\":4}\n",
    "def answer_mcq_graph(question, choices):\n",
    "    \"\"\"\n",
    "    Multiple Choice QA with Wikipedia + Hybrid Retrieval.\n",
    "    \n",
    "    Steps:\n",
    "    1. Use queries (from rephrase/expand/decompose chains).\n",
    "    2. Retrieve Wikipedia text for all queries (multi_wiki_search).\n",
    "    3. Split + hybrid index, then retrieve top chunks for the rephrased query.\n",
    "    4. Compare each option against the retrieved evidence.\n",
    "    5. Return the best option + detailed scores.\n",
    "    \"\"\"\n",
    "     \n",
    "    print(f\"question: {question}, \\n chouces: {choices} \\n ----------------\", )\n",
    "    \n",
    "    answer = first_guess_chain.invoke({\"question\": question, \"choices\": choices})\n",
    "    print(f\"answer: {answer} \\n ---------------\")\n",
    "    \n",
    "    rephrased = rephrase_chain.invoke({\"question\": question})\n",
    "    print(f\"rephrased: {rephrased} \\n ---------------\")\n",
    "    \n",
    "    letter_to_number = {\"A\":0, \"B\":1, \"C\":2, \"D\":3, \"E\":4}\n",
    "    expanded = expand_chain.invoke({\"answer\": choices[letter_to_number[re.search(r'\\b[A-E]\\b', answer).group()]], \"question\": question})\n",
    "    print(f\"expanded: {expanded} \\n ---------------\")\n",
    "    # decomposed = decompose_chain.invoke({\"question\": question})\n",
    "    # print(f\"decomposed: {decomposed} \\n ---------------\")\n",
    "    \n",
    "    clean_expanded = ''\n",
    "    is_brace_open = False\n",
    "    for i in expanded:\n",
    "        if i == '[' or is_brace_open:\n",
    "            clean_expanded += i \n",
    "            is_brace_open = True\n",
    "        if i == ']':\n",
    "            break\n",
    "    print(f\"expanded: {clean_expanded} \\n ---------------\")\n",
    "    all_queries = [rephrased.replace('\\n', '')] + ast.literal_eval(clean_expanded)\n",
    "    if all_queries[0] == all_queries[1]:\n",
    "        all_queries = all_queries[1:]\n",
    "    print(f\"all queries: {all_queries} \\n -------------------------\")\n",
    "\n",
    "    # 1. Gather Wikipedia text from all queries\n",
    "    wiki_text = multi_wiki_search(all_queries)\n",
    "    if not wiki_text.strip():\n",
    "        return None, {opt: \"NO EVIDENCE\" for opt in choices}\n",
    "\n",
    "    # 2. Retrieve top chunks using hybrid retriever\n",
    "    top_chunks = retrieve_top_chunks(wiki_text, rephrased, k=5)\n",
    "    \n",
    "    for chunk in top_chunks:\n",
    "        print(f\"chunk: {chunk}\")\n",
    "    print(\"--------------------------------\")\n",
    "    # 3. Ask comparator LLM to judge each option against evidence\n",
    "    result = compare_chain.invoke({\n",
    "        \"question\": question,\n",
    "        \"choices\": choices,\n",
    "        \"snippets\": top_chunks\n",
    "    })\n",
    "    \n",
    "    print(f\"result: {result} \\n -------------------------\")\n",
    "    # 4. Parse answer\n",
    "    best_answer = result.strip().upper()\n",
    "    # Remove punctuation and extra words\n",
    "    best_answer = best_answer.replace(\".\", \"\").replace(\"ANSWER:\", \"\").strip()\n",
    "    # Keep only first valid letter\n",
    "    if best_answer and best_answer[0] in \"ABCDE\":\n",
    "        best_answer = best_answer[0]\n",
    "    else:\n",
    "        return None, {c: \"UNKNOWN\" for c in choices}\n",
    "\n",
    "    # 5. Map back to choices\n",
    "    scores = {c: (\"YES\" if i == \"ABCDE\".index(best_answer) else \"NO\")\n",
    "              for i, c in enumerate(choices)}\n",
    "\n",
    "    return choices[\"ABCDE\".index(best_answer)], scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4634c94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: What is the main sequence in astronomy?, \n",
      " chouces: ['A group of galaxies', 'A type of telescope', 'A phase in stellar evolution', 'A planetary orbit', 'A black hole classification'] \n",
      " ----------------\n",
      "answer: \n",
      "\n",
      "                                 A \n",
      " ---------------\n",
      "rephrased: \n",
      "\n",
      "What is the main sequence in astronomy? \n",
      " ---------------\n",
      "expanded: \n",
      "\n",
      "[\"main sequence star\", \"Hertzsprung-Russell diagram\"] \n",
      " ---------------\n",
      "expanded: [\"main sequence star\", \"Hertzsprung-Russell diagram\"] \n",
      " ---------------\n",
      "all queries: ['What is the main sequence in astronomy?', 'main sequence star', 'Hertzsprung-Russell diagram'] \n",
      " -------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b283a222c1470bba22ca9e40ac6fcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IrajGharaee\\AppData\\Roaming\\Python\\Python313\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\IrajGharaee\\.cache\\huggingface\\hub\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "166c5d62bda64567b6d840056bfb1018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16e3094ec2434f539210973705d4e006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d0252f1c5734a4da4e9597248bee51a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "053846d1a3b6452899e0b6c494961d7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e0c328005f04fe5bb7e8378938699e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 10\u001b[0m\n\u001b[0;32m      1\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the main sequence in astronomy?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m choices \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA group of galaxies\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA type of telescope\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA black hole classification\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m ]\n\u001b[1;32m---> 10\u001b[0m answer \u001b[38;5;241m=\u001b[39m answer_mcq_graph(question, choices)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted answer:\u001b[39m\u001b[38;5;124m\"\u001b[39m, answer)\n",
      "Cell \u001b[1;32mIn[16], line 48\u001b[0m, in \u001b[0;36manswer_mcq_graph\u001b[1;34m(question, choices)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, {opt: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNO EVIDENCE\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m opt \u001b[38;5;129;01min\u001b[39;00m choices}\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# 2. Retrieve top chunks using hybrid retriever\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m top_chunks \u001b[38;5;241m=\u001b[39m retrieve_top_chunks(wiki_text, rephrased, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m top_chunks:\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 62\u001b[0m, in \u001b[0;36mretrieve_top_chunks\u001b[1;34m(text, query, k)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mretrieve_top_chunks\u001b[39m(text, query, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m---> 62\u001b[0m     retriever \u001b[38;5;241m=\u001b[39m build_hybrid_retriever(text)\n\u001b[0;32m     63\u001b[0m     docs \u001b[38;5;241m=\u001b[39m retriever\u001b[38;5;241m.\u001b[39mget_relevant_documents(query)\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs[:k]]\n",
      "Cell \u001b[1;32mIn[5], line 54\u001b[0m, in \u001b[0;36mbuild_hybrid_retriever\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbuild_hybrid_retriever\u001b[39m(text):\n\u001b[1;32m---> 54\u001b[0m     text_splitter \u001b[38;5;241m=\u001b[39m SentenceTransformersTokenTextSplitter(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     55\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39msplit_text(text)\n\u001b[0;32m     56\u001b[0m     bm25 \u001b[38;5;241m=\u001b[39m BM25Retriever\u001b[38;5;241m.\u001b[39mfrom_texts(sentences)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\langchain_text_splitters\\sentence_transformers.py:32\u001b[0m, in \u001b[0;36mSentenceTransformersTokenTextSplitter.__init__\u001b[1;34m(self, chunk_overlap, model_name, tokens_per_chunk, **kwargs)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;241m=\u001b[39m model_name\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m SentenceTransformer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mtokenizer\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_chunk_configuration(tokens_per_chunk\u001b[38;5;241m=\u001b[39mtokens_per_chunk)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sentence_transformers\\SentenceTransformer.py:327\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[1;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[0m\n\u001b[0;32m    309\u001b[0m has_modules \u001b[38;5;241m=\u001b[39m is_sentence_transformer_model(\n\u001b[0;32m    310\u001b[0m     model_name_or_path,\n\u001b[0;32m    311\u001b[0m     token,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    314\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    315\u001b[0m )\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    317\u001b[0m     has_modules\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_model_type(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    326\u001b[0m ):\n\u001b[1;32m--> 327\u001b[0m     modules, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_sbert_model(\n\u001b[0;32m    328\u001b[0m         model_name_or_path,\n\u001b[0;32m    329\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m    330\u001b[0m         cache_folder\u001b[38;5;241m=\u001b[39mcache_folder,\n\u001b[0;32m    331\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m    332\u001b[0m         trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[0;32m    333\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    334\u001b[0m         model_kwargs\u001b[38;5;241m=\u001b[39mmodel_kwargs,\n\u001b[0;32m    335\u001b[0m         tokenizer_kwargs\u001b[38;5;241m=\u001b[39mtokenizer_kwargs,\n\u001b[0;32m    336\u001b[0m         config_kwargs\u001b[38;5;241m=\u001b[39mconfig_kwargs,\n\u001b[0;32m    337\u001b[0m     )\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    339\u001b[0m     modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_auto_model(\n\u001b[0;32m    340\u001b[0m         model_name_or_path,\n\u001b[0;32m    341\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    349\u001b[0m         has_modules\u001b[38;5;241m=\u001b[39mhas_modules,\n\u001b[0;32m    350\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sentence_transformers\\SentenceTransformer.py:2253\u001b[0m, in \u001b[0;36mSentenceTransformer._load_sbert_model\u001b[1;34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[0m\n\u001b[0;32m   2248\u001b[0m         module \u001b[38;5;241m=\u001b[39m module_class\u001b[38;5;241m.\u001b[39mload(local_path)\n\u001b[0;32m   2250\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2251\u001b[0m     \u001b[38;5;66;03m# Newer modules that support the new loading method are loaded with the new style\u001b[39;00m\n\u001b[0;32m   2252\u001b[0m     \u001b[38;5;66;03m# i.e. with many keyword arguments that can optionally be used by the modules\u001b[39;00m\n\u001b[1;32m-> 2253\u001b[0m     module \u001b[38;5;241m=\u001b[39m module_class\u001b[38;5;241m.\u001b[39mload(\n\u001b[0;32m   2254\u001b[0m         model_name_or_path,\n\u001b[0;32m   2255\u001b[0m         \u001b[38;5;66;03m# Loading-specific keyword arguments\u001b[39;00m\n\u001b[0;32m   2256\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39mmodule_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   2257\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   2258\u001b[0m         cache_folder\u001b[38;5;241m=\u001b[39mcache_folder,\n\u001b[0;32m   2259\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m   2260\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   2261\u001b[0m         \u001b[38;5;66;03m# Module-specific keyword arguments\u001b[39;00m\n\u001b[0;32m   2262\u001b[0m         trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[0;32m   2263\u001b[0m         model_kwargs\u001b[38;5;241m=\u001b[39mmodel_kwargs,\n\u001b[0;32m   2264\u001b[0m         tokenizer_kwargs\u001b[38;5;241m=\u001b[39mtokenizer_kwargs,\n\u001b[0;32m   2265\u001b[0m         config_kwargs\u001b[38;5;241m=\u001b[39mconfig_kwargs,\n\u001b[0;32m   2266\u001b[0m         backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend,\n\u001b[0;32m   2267\u001b[0m     )\n\u001b[0;32m   2269\u001b[0m modules[module_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m module\n\u001b[0;32m   2270\u001b[0m module_kwargs[module_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m module_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sentence_transformers\\models\\Transformer.py:338\u001b[0m, in \u001b[0;36mTransformer.load\u001b[1;34m(cls, model_name_or_path, subfolder, token, cache_folder, revision, local_files_only, trust_remote_code, model_kwargs, tokenizer_kwargs, config_kwargs, backend, **kwargs)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    324\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m    325\u001b[0m     init_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_load_init_kwargs(\n\u001b[0;32m    326\u001b[0m         model_name_or_path\u001b[38;5;241m=\u001b[39mmodel_name_or_path,\n\u001b[0;32m    327\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39msubfolder,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    336\u001b[0m         backend\u001b[38;5;241m=\u001b[39mbackend,\n\u001b[0;32m    337\u001b[0m     )\n\u001b[1;32m--> 338\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(model_name_or_path\u001b[38;5;241m=\u001b[39mmodel_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minit_kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sentence_transformers\\models\\Transformer.py:87\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[1;34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path, backend)\u001b[0m\n\u001b[0;32m     84\u001b[0m     config_args \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     86\u001b[0m config, is_peft_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_config(model_name_or_path, cache_dir, backend, config_args)\n\u001b[1;32m---> 87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_model(model_name_or_path, config, cache_dir, backend, is_peft_model, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_max_length\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokenizer_args:\n\u001b[0;32m     90\u001b[0m     tokenizer_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_max_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m max_seq_length\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sentence_transformers\\models\\Transformer.py:185\u001b[0m, in \u001b[0;36mTransformer._load_model\u001b[1;34m(self, model_name_or_path, config, cache_dir, backend, is_peft_model, **model_args)\u001b[0m\n\u001b[0;32m    183\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_mt5_model(model_name_or_path, config, cache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args)\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 185\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model \u001b[38;5;241m=\u001b[39m AutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    186\u001b[0m             model_name_or_path, config\u001b[38;5;241m=\u001b[39mconfig, cache_dir\u001b[38;5;241m=\u001b[39mcache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args\n\u001b[0;32m    187\u001b[0m         )\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m backend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monnx\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model \u001b[38;5;241m=\u001b[39m load_onnx_model(\n\u001b[0;32m    190\u001b[0m         model_name_or_path\u001b[38;5;241m=\u001b[39mmodel_name_or_path,\n\u001b[0;32m    191\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m    192\u001b[0m         task_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature-extraction\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    193\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args,\n\u001b[0;32m    194\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\models\\auto\\auto_factory.py:604\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    602\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    603\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[1;32m--> 604\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    605\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    606\u001b[0m     )\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    609\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    610\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\modeling_utils.py:288\u001b[0m, in \u001b[0;36mrestore_default_dtype.<locals>._wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    286\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    290\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\modeling_utils.py:5027\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   5017\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   5018\u001b[0m     gguf_file\n\u001b[0;32m   5019\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   5020\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m ((\u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map)\n\u001b[0;32m   5021\u001b[0m ):\n\u001b[0;32m   5022\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   5023\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOne or more modules is configured to be mapped to disk. Disk offload is not supported for models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   5024\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloaded from GGUF files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   5025\u001b[0m     )\n\u001b[1;32m-> 5027\u001b[0m checkpoint_files, sharded_metadata \u001b[38;5;241m=\u001b[39m _get_resolved_checkpoint_files(\n\u001b[0;32m   5028\u001b[0m     pretrained_model_name_or_path\u001b[38;5;241m=\u001b[39mpretrained_model_name_or_path,\n\u001b[0;32m   5029\u001b[0m     subfolder\u001b[38;5;241m=\u001b[39msubfolder,\n\u001b[0;32m   5030\u001b[0m     variant\u001b[38;5;241m=\u001b[39mvariant,\n\u001b[0;32m   5031\u001b[0m     gguf_file\u001b[38;5;241m=\u001b[39mgguf_file,\n\u001b[0;32m   5032\u001b[0m     from_tf\u001b[38;5;241m=\u001b[39mfrom_tf,\n\u001b[0;32m   5033\u001b[0m     from_flax\u001b[38;5;241m=\u001b[39mfrom_flax,\n\u001b[0;32m   5034\u001b[0m     use_safetensors\u001b[38;5;241m=\u001b[39muse_safetensors,\n\u001b[0;32m   5035\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   5036\u001b[0m     force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m   5037\u001b[0m     proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   5038\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   5039\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   5040\u001b[0m     user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[0;32m   5041\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m   5042\u001b[0m     commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[0;32m   5043\u001b[0m     is_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_auto_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   5044\u001b[0m     transformers_explicit_filename\u001b[38;5;241m=\u001b[39mtransformers_explicit_filename,\n\u001b[0;32m   5045\u001b[0m )\n\u001b[0;32m   5047\u001b[0m is_sharded \u001b[38;5;241m=\u001b[39m sharded_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   5048\u001b[0m is_quantized \u001b[38;5;241m=\u001b[39m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\modeling_utils.py:1150\u001b[0m, in \u001b[0;36m_get_resolved_checkpoint_files\u001b[1;34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash, is_remote_code, transformers_explicit_filename)\u001b[0m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1136\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m     cached_file_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1138\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_dir,\n\u001b[0;32m   1139\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforce_download\u001b[39m\u001b[38;5;124m\"\u001b[39m: force_download,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1148\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m: commit_hash,\n\u001b[0;32m   1149\u001b[0m     }\n\u001b[1;32m-> 1150\u001b[0m     resolved_archive_file \u001b[38;5;241m=\u001b[39m cached_file(pretrained_model_name_or_path, filename, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcached_file_kwargs)\n\u001b[0;32m   1152\u001b[0m     \u001b[38;5;66;03m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;66;03m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[0;32m   1154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename \u001b[38;5;241m==\u001b[39m _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001b[0;32m   1155\u001b[0m         \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\utils\\hub.py:321\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcached_file\u001b[39m(\n\u001b[0;32m    264\u001b[0m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[0;32m    265\u001b[0m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    267\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    268\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;124;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[0;32m    270\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 321\u001b[0m     file \u001b[38;5;241m=\u001b[39m cached_files(path_or_repo_id\u001b[38;5;241m=\u001b[39mpath_or_repo_id, filenames\u001b[38;5;241m=\u001b[39m[filename], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    322\u001b[0m     file \u001b[38;5;241m=\u001b[39m file[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\utils\\hub.py:478\u001b[0m, in \u001b[0;36mcached_files\u001b[1;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    477\u001b[0m         \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[1;32m--> 478\u001b[0m         hf_hub_download(\n\u001b[0;32m    479\u001b[0m             path_or_repo_id,\n\u001b[0;32m    480\u001b[0m             filenames[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    481\u001b[0m             subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(subfolder) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m subfolder,\n\u001b[0;32m    482\u001b[0m             repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[0;32m    483\u001b[0m             revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m    484\u001b[0m             cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m    485\u001b[0m             user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[0;32m    486\u001b[0m             force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m    487\u001b[0m             proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m    488\u001b[0m             resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[0;32m    489\u001b[0m             token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m    490\u001b[0m             local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    491\u001b[0m         )\n\u001b[0;32m    492\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    493\u001b[0m         snapshot_download(\n\u001b[0;32m    494\u001b[0m             path_or_repo_id,\n\u001b[0;32m    495\u001b[0m             allow_patterns\u001b[38;5;241m=\u001b[39mfull_filenames,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    504\u001b[0m             local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    505\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\huggingface_hub\\file_download.py:1010\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m    990\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[0;32m    991\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[0;32m    992\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1007\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   1008\u001b[0m     )\n\u001b[0;32m   1009\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1010\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_cache_dir(\n\u001b[0;32m   1011\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[0;32m   1012\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   1013\u001b[0m         \u001b[38;5;66;03m# File info\u001b[39;00m\n\u001b[0;32m   1014\u001b[0m         repo_id\u001b[38;5;241m=\u001b[39mrepo_id,\n\u001b[0;32m   1015\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[0;32m   1016\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[0;32m   1017\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m   1018\u001b[0m         \u001b[38;5;66;03m# HTTP info\u001b[39;00m\n\u001b[0;32m   1019\u001b[0m         endpoint\u001b[38;5;241m=\u001b[39mendpoint,\n\u001b[0;32m   1020\u001b[0m         etag_timeout\u001b[38;5;241m=\u001b[39metag_timeout,\n\u001b[0;32m   1021\u001b[0m         headers\u001b[38;5;241m=\u001b[39mhf_headers,\n\u001b[0;32m   1022\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   1023\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   1024\u001b[0m         \u001b[38;5;66;03m# Additional options\u001b[39;00m\n\u001b[0;32m   1025\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   1026\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m   1027\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\huggingface_hub\\file_download.py:1171\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[1;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[0;32m   1168\u001b[0m \u001b[38;5;66;03m# Local file doesn't exist or etag isn't a match => retrieve file from remote (or cache)\u001b[39;00m\n\u001b[0;32m   1170\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[1;32m-> 1171\u001b[0m     _download_to_tmp_and_move(\n\u001b[0;32m   1172\u001b[0m         incomplete_path\u001b[38;5;241m=\u001b[39mPath(blob_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.incomplete\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1173\u001b[0m         destination_path\u001b[38;5;241m=\u001b[39mPath(blob_path),\n\u001b[0;32m   1174\u001b[0m         url_to_download\u001b[38;5;241m=\u001b[39murl_to_download,\n\u001b[0;32m   1175\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   1176\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   1177\u001b[0m         expected_size\u001b[38;5;241m=\u001b[39mexpected_size,\n\u001b[0;32m   1178\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[0;32m   1179\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m   1180\u001b[0m         etag\u001b[38;5;241m=\u001b[39metag,\n\u001b[0;32m   1181\u001b[0m         xet_file_data\u001b[38;5;241m=\u001b[39mxet_file_data,\n\u001b[0;32m   1182\u001b[0m     )\n\u001b[0;32m   1183\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pointer_path):\n\u001b[0;32m   1184\u001b[0m         _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\huggingface_hub\\file_download.py:1738\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[1;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download, etag, xet_file_data)\u001b[0m\n\u001b[0;32m   1731\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m constants\u001b[38;5;241m.\u001b[39mHF_HUB_DISABLE_XET:\n\u001b[0;32m   1732\u001b[0m             logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m   1733\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXet Storage is enabled for this repo, but the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhf_xet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m package is not installed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1734\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFalling back to regular HTTP download. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1735\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1736\u001b[0m             )\n\u001b[1;32m-> 1738\u001b[0m         http_get(\n\u001b[0;32m   1739\u001b[0m             url_to_download,\n\u001b[0;32m   1740\u001b[0m             f,\n\u001b[0;32m   1741\u001b[0m             proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   1742\u001b[0m             resume_size\u001b[38;5;241m=\u001b[39mresume_size,\n\u001b[0;32m   1743\u001b[0m             headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   1744\u001b[0m             expected_size\u001b[38;5;241m=\u001b[39mexpected_size,\n\u001b[0;32m   1745\u001b[0m         )\n\u001b[0;32m   1747\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1748\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\huggingface_hub\\file_download.py:496\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[0;32m    494\u001b[0m new_resume_size \u001b[38;5;241m=\u001b[39m resume_size\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 496\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39mconstants\u001b[38;5;241m.\u001b[39mDOWNLOAD_CHUNK_SIZE):\n\u001b[0;32m    497\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[0;32m    498\u001b[0m             progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\requests\\models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32me:\\Conda\\Lib\\site-packages\\urllib3\\response.py:1066\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1064\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1065\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1066\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(amt\u001b[38;5;241m=\u001b[39mamt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[0;32m   1068\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[0;32m   1069\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[1;32me:\\Conda\\Lib\\site-packages\\urllib3\\response.py:955\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    952\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[0;32m    953\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[1;32m--> 955\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raw_read(amt)\n\u001b[0;32m    957\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[0;32m    959\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32me:\\Conda\\Lib\\site-packages\\urllib3\\response.py:879\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    876\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    878\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 879\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp_read(amt, read1\u001b[38;5;241m=\u001b[39mread1) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    887\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[0;32m    888\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[0;32m    889\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32me:\\Conda\\Lib\\site-packages\\urllib3\\response.py:862\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[0;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32me:\\Conda\\Lib\\http\\client.py:479\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    478\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[1;32m--> 479\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[0;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[0;32m    481\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32me:\\Conda\\Lib\\socket.py:719\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    717\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot read from timed out object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv_into(b)\n\u001b[0;32m    720\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    721\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32me:\\Conda\\Lib\\ssl.py:1304\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1300\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1301\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1302\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1303\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(nbytes, buffer)\n\u001b[0;32m   1305\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32me:\\Conda\\Lib\\ssl.py:1138\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1138\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1139\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1140\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "question = \"What is the main sequence in astronomy?\"\n",
    "choices = [\n",
    "    \"A group of galaxies\",\n",
    "    \"A type of telescope\",\n",
    "    \"A phase in stellar evolution\",\n",
    "    \"A planetary orbit\",\n",
    "    \"A black hole classification\"\n",
    "]\n",
    "\n",
    "answer = answer_mcq_graph(question, choices)\n",
    "print(\"Predicted answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5c3fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = pd.read_csv(\"data/train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3467668",
   "metadata": {},
   "outputs": [],
   "source": [
    "Choices = []\n",
    "Questions = []\n",
    "Answers = []\n",
    "for index, row in train_file.iterrows():\n",
    "    Choices.append([row['A'], row['B'], row['C'], row['D'], row['E']])\n",
    "    Questions.append(row['prompt'])\n",
    "    Answers.append(row['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752f8e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd9ed36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: What is the role of methane in Fischer-Tropsch processes? \n",
      " ----------------\n",
      "['Methane is partially converted to carbon monoxide for utilization in Fischer-Tropsch processes.', 'Methane is used as a catalyst in Fischer-Tropsch processes.', 'Methane is not used in Fischer-Tropsch processes.', 'Methane is fully converted to carbon monoxide for utilization in Fischer-Tropsch processes.', 'Methane is a byproduct of Fischer-Tropsch processes.']\n",
      "answer: \n",
      "\n",
      "A. \n",
      " ---------------\n",
      "rephrased: \n",
      "\n",
      "\"Fischer-Tropsch process methane role\" \n",
      " ---------------\n",
      "expanded: \n",
      "\n",
      "[\"Fischer-Tropsch process and methane conversion\", \"Role of methane in syngas production\"] \n",
      " ---------------\n",
      "expanded: [\"Fischer-Tropsch process and methane conversion\", \"Role of methane in syngas production\"] \n",
      " ---------------\n",
      "all queries: ['\"Fischer-Tropsch process methane role\"', 'Fischer-Tropsch process and methane conversion', 'Role of methane in syngas production'] \n",
      " -------------------------\n",
      "wiki_text: No good Wikipedia Search Result was found\n",
      "\n",
      "Page: Fischer–Tropsch process\n",
      "Summary: The Fischer–Tropsch process (FT) is a collection of chemical reactions that converts a mixture of carbon monoxide and hydrogen, known as syngas, into liquid hydrocarbons. These reactions occur in the presence of metal catalysts, typically at temperatures of 150–300 °C (302–572 °F) and pressures of one to several tens of atmospheres. The Fischer–Tropsch process is an important reaction in both coal liquefaction and gas to liquids technology for producing liquid hydrocarbons.\n",
      "In the usual implementation, carbon monoxide and hydrogen, the feedstocks for FT, are produced from coal, natural gas, or biomass in a process known as gasification. The process then converts these gases into synthetic lubrication oil and synthetic fuel. This process has received intermittent attention as a source of low-sulfur diesel fuel and to address the supply or cost of petroleum-derived hydrocarbons. Fischer–Tropsch process is discussed as a step of producing carbon-neutral liquid hydrocarbon fuels from CO2 and hydrogen.\n",
      "The process was first developed by Franz Fischer and Hans Tropsch at the Kaiser Wilhelm Institute for Coal Research in Mülheim an der Ruhr, Germany, in 1925.\n",
      "\n",
      "Page: Gas to liquids\n",
      "Summary: Gas to liquids (GTL) is a refinery process to convert natural gas or other gaseous hydrocarbons into longer-chain hydrocarbons, such as gasoline or diesel fuel. Methane-rich gases are converted into liquid synthetic fuels. Two general strategies exist: (i) direct partial combustion of methane to methanol and (ii) Fischer–Tropsch-like processes that convert carbon monoxide and hydrogen into hydrocarbons. Strategy ii is followed by diverse methods to convert the hydrogen-carbon monoxide mixtures to liquids. Direct partial combustion has been demonstrated in nature but not replicated commercially.  Technologies reliant on partial combustion have been commercialized mainly in regions where natural gas is inexpensive.\n",
      "The motivation for GTL is to produce liquid fuels, which are more readily transported than methane.  Methane must be cooled below its critical temperature of −82.3 °C in order to be liquified under pressure.  Because of the associated cryogenic apparatus, LNG tankers are used for transport. Methanol is a conveniently handled combustible liquid, but its energy density is half of that of gasoline.\n",
      "\n",
      "Page: Haber process\n",
      "Summary: The Haber process, also called the Haber–Bosch process, is the main industrial procedure for the production of ammonia. It converts atmospheric nitrogen (N2) to ammonia (NH3) by a reaction with hydrogen (H2) using finely divided iron metal as a catalyst:\n",
      "\n",
      "  \n",
      "    \n",
      "      \n",
      "        \n",
      "          \n",
      "            N\n",
      "            \n",
      "              2\n",
      "            \n",
      "            \n",
      "              \n",
      "            \n",
      "          \n",
      "          +\n",
      "          3\n",
      "          \n",
      "          \n",
      "            H\n",
      "            \n",
      "              2\n",
      "            \n",
      "            \n",
      "              \n",
      "            \n",
      "          \n",
      "          \n",
      "            \n",
      "              \n",
      "                \n",
      "                  \n",
      "                    \n",
      "                      ↽\n",
      "                    \n",
      "                    \n",
      "                    \n",
      "                    \n",
      "                      −\n",
      "                    \n",
      "                  \n",
      "                \n",
      "              \n",
      "              \n",
      "                \n",
      "                  \n",
      "                    \n",
      "                      −\n",
      "                    \n",
      "                    \n",
      "                    \n",
      "                    \n",
      "                      ⇀\n",
      "                    \n",
      "                  \n",
      "                \n",
      "              \n",
      "            \n",
      "          \n",
      "          2\n",
      "          \n",
      "          \n",
      "            NH\n",
      "            \n",
      "              3\n",
      "            \n",
      "            \n",
      "              \n",
      "            \n",
      "          \n",
      "        \n",
      "        \n",
      "        \n",
      "          Δ\n",
      "          \n",
      "            H\n",
      "            \n",
      "              \n",
      "                298\n",
      "                 \n",
      "                K\n",
      "              \n",
      "            \n",
      "            \n",
      "              ∘\n",
      "            \n",
      "          \n",
      "          =\n",
      "          −\n",
      "          92.28\n",
      "           \n",
      "          \n",
      "            kJ per \n",
      "\n",
      "Page: Hydrogen production\n",
      "Summary: Hydrogen gas is produced by several industrial methods. Nearly all of the world's current supply of hydrogen is created from fossil fuels. Most hydrogen is gray hydrogen made through steam methane reforming. In this process, hydrogen is produced from a chemical reaction between steam and methane, the main component of natural gas. Producing one tonne of hydrogen through this process emits 6.6–9.3 tonnes of carbon dioxide. When carbon capture and storage is used to remove a large fraction of these emissions, the product is known as blue hydrogen.\n",
      "Green hydrogen is usually understood to be produced from renewable electricity via electrolysis of water. Less frequently, definitions of green hydrogen  include hydrogen produced from other low-emission sources such as biomass. Producing green hydrogen is currently more expensive than producing gray hydrogen, and the efficiency of energy conversion is inherently low. Other methods of hydrogen production include biomass gasification, methane pyrolysis, extraction of underground natural hydrogen, and in situ hydrogen synthesis.\n",
      "As of 2023, less than 1% of dedicated hydrogen production is low-carbon, i.e. blue hydrogen, green hydrogen, and hydrogen produced from biomass.\n",
      "In 2020, roughly 87 million tons of hydrogen was produced worldwide for various uses, such as oil refining, in the production of ammonia through the Haber process, and in the production of methanol through reduction of carbon monoxide. The global hydrogen generation market was fairly valued at US$155 billion in 2022, and expected to grow at a compound annual growth rate of 9.3% from 2023 to 2030.\n",
      "\n",
      "Page: Biogas\n",
      "Summary: Biogas is a gaseous renewable energy source produced from raw materials such as agricultural waste, manure, municipal waste, plant material, sewage, green waste, wastewater, and food waste. Biogas is produced by anaerobic digestion with anaerobic organisms or methanogens inside an anaerobic digester, biodigester or a bioreactor.\n",
      "The gas composition is primarily methane (CH4) and carbon dioxide (CO2) and may have small amounts of hydrogen sulfide (H2S), moisture and siloxanes. The methane can be combusted or oxidized with oxygen. This energy release allows biogas to be used as a fuel; it can be used in fuel cells and for heating purpose, such as in cooking. It can also be used in a gas engine to convert the energy in the gas into electricity and heat.\n",
      "After removal of carbon dioxide and hydrogen sulfide it can be compressed in the same way as natural gas and used to power motor vehicles. In the United Kingdom, for example, biogas is estimated to have the potential to replace around 17% of vehicle fuel. It qualifies for renewable energy subsidies in some parts of the world. Biogas can be cleaned and upgraded to natural gas standards, when it becomes bio-methane. Biogas is considered to be a renewable resource because its production-and-use cycle is continuous, and it generates no net carbon dioxide. From a carbon perspective, as much carbon dioxide is absorbed from the atmosphere in the growth of the primary bio-resource as is released, when the material is ultimately converted to energy.\n",
      "\n",
      "Page: Power-to-gas\n",
      "Summary: Power-to-gas (often abbreviated P2G) is a technology that uses electric power to produce a gaseous fuel.\n",
      "Most P2G systems use electrolysis to produce hydrogen. The hydrogen can be used directly, or further steps (known as two-stage P2G systems) may convert the hydrogen into syngas, methane, or LPG. \n",
      "Single-stage P2G systems to produce methane also exist, such as reversible solid oxide cell (rSOC) technology.\n",
      "Produced gas, just like natural gas or industrially produced hydrogen or methane, is a commodity and may be used as such through existing infrastructure (pipelines and gas storage facilities), including back to power at a loss. However, provided the power comes from renewable energy, it can be touted as a carbon-neutral fuel, renewable, and a way to store v \n",
      " -------------------------\n",
      "top_chunks: ['Page: Fischer–Tropsch process', 'Summary: Gas to liquids (GTL) is a refinery process to convert natural gas or other gaseous hydrocarbons into longer-chain hydrocarbons, such as gasoline or diesel fuel. Methane-rich gases are converted into liquid synthetic fuels. Two general strategies exist: (i) direct partial combustion of methane to methanol and (ii) Fischer–Tropsch-like processes that convert carbon monoxide and hydrogen into hydrocarbons. Strategy ii is followed by diverse methods to convert the hydrogen-carbon monoxide', 'In the usual implementation, carbon monoxide and hydrogen, the feedstocks for FT, are produced from coal, natural gas, or biomass in a process known as gasification. The process then converts these gases into synthetic lubrication oil and synthetic fuel. This process has received intermittent attention as a source of low-sulfur diesel fuel and to address the supply or cost of petroleum-derived hydrocarbons. Fischer–Tropsch process is discussed as a step of producing carbon-neutral liquid'] \n",
      " -------------------------\n",
      "question: What is the formalism that angular momentum is associated with in rotational invariance? \n",
      " ----------------\n",
      "['Angular momentum is the 1-form Noether charge associated with rotational invariance.', 'Angular momentum is the 3-form Noether charge associated with rotational invariance.', 'Angular momentum is the 5-form Noether charge associated with rotational invariance.', 'Angular momentum is the 2-form Noether charge associated with rotational invariance.', 'Angular momentum is the 4-form Noether charge associated with rotational invariance.']\n",
      "answer: \n",
      "\n",
      "A\n",
      "\n",
      "Explanation: The 1-form is associated with time translation invariance, the 2-form is associated with spatial translation invariance, the 3-form is associated with rotational invariance, the 4-form is associated with time reversal invariance, and the 5-form is associated with Lorentz invariance. \n",
      " ---------------\n",
      "rephrased: \n",
      "\n",
      "angular momentum formalism in rotational invariance \n",
      " ---------------\n",
      "expanded: \n",
      "\n",
      "[\"Noether's theorem\", \"Noether charge\"] \n",
      " ---------------\n",
      "expanded: [\"Noether's theorem\", \"Noether charge\"] \n",
      " ---------------\n",
      "all queries: ['angular momentum formalism in rotational invariance', \"Noether's theorem\", 'Noether charge'] \n",
      " -------------------------\n",
      "wiki_text: Page: Angular momentum\n",
      "Summary: Angular momentum (sometimes called moment of momentum or rotational momentum) is the rotational analog of linear momentum. It is an important physical quantity because it is a conserved quantity – the total angular momentum of a closed system remains constant.  Angular momentum has both a direction and a magnitude, and both are conserved.  Bicycles and motorcycles, flying discs, rifled bullets, and gyroscopes owe their useful properties to conservation of angular momentum.  Conservation of angular momentum is also why hurricanes form spirals and neutron stars have high rotational rates.  In general, conservation limits the possible motion of a system, but it does not uniquely determine it.\n",
      "The three-dimensional angular momentum for a point particle is classically represented as a pseudovector r × p, the cross product of the particle's position vector r (relative to some origin) and its momentum vector; the latter is p = mv in Newtonian mechanics. Unlike linear momentum, angular momentum depends on where this origin is chosen, since the particle's position is measured from it.\n",
      "Angular momentum is an extensive quantity; that is, the total angular momentum of any composite system is the sum of the angular momenta of its constituent parts. For a continuous rigid body or a fluid, the total angular momentum is the volume integral of angular momentum density (angular momentum per unit volume in the limit as volume shrinks to zero) over the entire body.\n",
      "Similar to conservation of linear momentum, where it is conserved if there is no external force, angular momentum is conserved if there is no external torque.  Torque can be defined as the rate of change of angular momentum, analogous to force. The net external torque on any system is always equal to the total torque on the system; the sum of all internal torques of any system is always 0 (this is the rotational analogue of Newton's third law of motion). Therefore, for a closed system (where there is no net external torque), the total torque on the system must be 0, which means that the total angular momentum of the system is constant.\n",
      "The change in angular momentum for a particular interaction is called angular impulse, sometimes twirl. Angular impulse is the angular analog of (linear) impulse.\n",
      "\n",
      "\n",
      "\n",
      "Page: Relativistic angular momentum\n",
      "Summary: In physics, relativistic angular momentum refers to the mathematical formalisms and physical concepts that define angular momentum in special relativity (SR) and general relativity (GR). The relativistic quantity is subtly different from the three-dimensional quantity in classical mechanics.\n",
      "Angular momentum is an important dynamical quantity derived from position and momentum. It is a measure of an object's rotational motion and resistance to changes in its rotation. Also, in the same way momentum conservation corresponds to translational symmetry, angular momentum conservation corresponds to rotational symmetry – the connection between symmetries and conservation laws is made by Noether's theorem. While these concepts were originally discovered in classical mechanics, they are also true and significant in special and general relativity. In terms of abstract algebra, the invariance of angular momentum, four-momentum, and other symmetries in spacetime, are described by the Lorentz group, or more generally the Poincaré group.\n",
      "Physical quantities that remain separate in classical physics are naturally combined in SR and GR by enforcing the postulates of relativity. Most notably, the space and time coordinates combine into the four-position, and energy and momentum combine into the four-momentum. The components of these four-vectors depend on the frame of reference used, and change under Lorentz transformations to other inertial frames or accelerated frames.\n",
      "Relativistic angular momentum is less obvious. The classical definition of angular momentum is the cross product of position x with momentum p to obtain a pseudove\n",
      "\n",
      "Page: Noether's theorem\n",
      "Summary: Noether's theorem states that every continuous symmetry of the action of a physical system with conservative forces has a corresponding conservation law. This is the first of two theorems (see Noether's second theorem) published by the mathematician Emmy Noether in 1918. The action of a physical system is the integral over time of a Lagrangian function, from which the system's behavior can be determined by the principle of least action. This theorem applies to continuous and smooth symmetries of physical space. Noether's formulation is quite general and has been applied across classical mechanics, high energy physics, and recently statistical mechanics.\n",
      "Noether's theorem is used in theoretical physics and the calculus of variations. It reveals the fundamental relation between the symmetries of a physical system and the conservation laws. It also made  modern theoretical physicists much more focused on symmetries of physical systems. A generalization of the formulations on constants of motion in Lagrangian and Hamiltonian mechanics (developed in 1788 and 1833, respectively), it does not apply to systems that cannot be modeled with a Lagrangian alone (e.g., systems with a Rayleigh dissipation function). In particular, dissipative systems with continuous symmetries need not have a corresponding conservation law.\n",
      "\n",
      "Page: Emmy Noether\n",
      "Summary: Amalie Emmy Noether (23 March 1882 – 14 April 1935) was a German mathematician who made many important contributions to abstract algebra. She also proved Noether's first and second theorems, which are fundamental in mathematical physics. Noether was described by Pavel Alexandrov, Albert Einstein, Jean Dieudonné, Hermann Weyl, and Norbert Wiener as the most important woman in the history of mathematics. As one of the leading mathematicians of her time, she developed theories of rings, fields, and algebras. In physics, Noether's theorem explains the connection between symmetry and conservation laws.\n",
      "Noether was born to a Jewish family in the Franconian town of Erlangen; her father was the mathematician Max Noether. She originally planned to teach French and English after passing the required examinations, but instead studied mathematics at the University of Erlangen–Nuremberg, where her father lectured. After completing her doctorate in 1907 under the supervision of Paul Gordan, she worked at the Mathematical Institute of Erlangen without pay for seven years. At the time, women were largely excluded from academic positions. In 1915, she was invited by David Hilbert and Felix Klein to join the mathematics department at the University of Göttingen, a world-renowned center of mathematical research. The philosophical faculty objected, and she spent four years lecturing under Hilbert's name. Her habilitation was approved in 1919, allowing her to obtain the rank of Privatdozent.\n",
      "Noether remained a leading member of the Göttingen mathematics department until 1933; her students were sometimes called the \"Noether Boys\". In 1924, Dutch mathematician B. L. van der Waerden joined her circle and soon became the leading expositor of Noether's ideas; her work was the foundation for the second volume of his influential 1931 textbook, Moderne Algebra. By the time of her plenary address at the 1932 International Congress of Mathematicians in Zürich, her algebraic acumen was recognized around the world. The following year, Germany's Nazi government dismissed Jews from university positions, and Noether moved to the United States to take up a position at Bryn Mawr College in Pennsylvania. There, she taught graduate and post-doctoral women including Marie Johanna Weiss and Olga Taussky-Todd. At the same time, she lectured and performed research at the Institute for Advanced Study in Princeton, New Jersey.\n",
      "Noether's mathematical work has been divided into three \"epochs\". In the first (1908–1919), she made contributions to the theories of algebraic invariants and number fields. Her work\n",
      "\n",
      "Page: Noether's theorem\n",
      "Summary: Noether's theorem states that every continuous symmetry of the action of a physical system with conservative forces has a corresponding conservation law. This is the first of two theorems (see Noether's second theorem) published by the mathematician Emmy Noether in 1918. The action of a physical system is the integral over time of a Lagrangian function, from which the system's behavior can be determined by the principle of least action. This theorem applies to continuous and smooth symmetries of physical space. Noether's formulation is quite general and has been applied across classical mechanics, high energy physics, and recently statistical mechanics.\n",
      "Noether's theorem is used in theoretical physics and the calculus of variations. It reveals the fundamental relation between the symmetries of a physical system and the conservation laws. It also made  modern theoretical physicists much more focused on symmetries of physical systems. A generalization of the formulations on constants of motion in Lagrangian and Hamiltonian mechanics (developed in 1788 and 1833, respectively), it does not apply to systems that cannot be modeled with a Lagrangian alone (e.g., systems with a Rayleigh dissipation function). In particular, dissipative systems with continuous symmetries need not have a corresponding conservation law.\n",
      "\n",
      "Page: Charge (physics)\n",
      "Summary: In physics, a charge is any of many different quantities, such as the electric charge in electromagnetism or the color charge in quantum chromodynamics. Charges correspond to the time-invariant generators of a symmetry group, and specifically, to the generators that commute with the Hamiltonian.  Charges are often denoted by ⁠\n",
      "  \n",
      "    \n",
      "      \n",
      "        Q\n",
      "      \n",
      "    \n",
      "    {\\displaystyle Q}\n",
      "  \n",
      "⁠, and so the invariance of the charge corresponds to the vanishing commutator ⁠\n",
      "  \n",
      "    \n",
      "      \n",
      "        [\n",
      "        Q\n",
      "        ,\n",
      "        H\n",
      "        ]\n",
      "        =\n",
      "        0\n",
      "      \n",
      "    \n",
      "    {\\displaystyle [Q,H]=0}\n",
      "  \n",
      "⁠, where \n",
      "  \n",
      "    \n",
      "      \n",
      "        H\n",
      "      \n",
      "    \n",
      "    {\\displaystyle H}\n",
      "  \n",
      " is the Hamiltonian. Thus, charges are associated with conserved quantum numbers; these are the eigenvalues of the generator \n",
      "  \n",
      "    \n",
      "      \n",
      "        Q\n",
      "      \n",
      "    \n",
      "    {\\displaystyle Q}\n",
      "  \n",
      ". A \"charge\" can also refer to a point-shaped object with an electric charge and a position, such as in the method of image charges.\n",
      "\n",
      "Page: Introduction to gauge theory\n",
      "Summary: A gauge theory is a type of theory in physics. The word gauge means a measurement, a thickness, an in-between distance (as in railroad tracks), or a resulting number of units per certain parameter (a number of loops in an inch of fabric or a number of lead balls in a pound of ammunition). Modern theories describe physical forces in terms of fields, e.g., the electromagnetic field, the gravitational field, and fields that describe forces between the elementary particles. A general feature of these field theories is that the fundamental fields cannot be directly measured; however, some associated quantities can be measured, such as charges, energies, and velocities. For example, say you cannot measure the diameter of a lead ball, but you can determine how many lead balls, which are equal in every way, are required to make a pound. Using the number of balls, the density of lead, and the formula for calculating the volume of a sphere from its diameter, one could indirectly determine the diameter of a single lead ball.\n",
      "In field theories, different configurations of the unobservable fields can result in identical observable quantities. A transformation from one such field configuration to another is called a gauge transformation; the lack of change in the measurable quantities, despite the field being transformed, is a property called gauge invariance. For example, if you could measure the color of lead balls and discover that when you change the color, you still fit the same number of balls in a pound, the property of \"color\" would show gauge i \n",
      " -------------------------\n",
      "top_chunks: [\"Angular momentum is an important dynamical quantity derived from position and momentum. It is a measure of an object's rotational motion and resistance to changes in its rotation. Also, in the same way momentum conservation corresponds to translational symmetry, angular momentum conservation corresponds to rotational symmetry – the connection between symmetries and conservation laws is made by Noether's theorem. While these concepts were originally discovered in classical mechanics, they are\", 'Summary: Angular momentum (sometimes called moment of momentum or rotational momentum) is the rotational analog of linear momentum. It is an important physical quantity because it is a conserved quantity – the total angular momentum of a closed system remains constant.  Angular momentum has both a direction and a magnitude, and both are conserved.  Bicycles and motorcycles, flying discs, rifled bullets, and gyroscopes owe their useful properties to conservation of angular momentum.', 'discovered in classical mechanics, they are also true and significant in special and general relativity. In terms of abstract algebra, the invariance of angular momentum, four-momentum, and other symmetries in spacetime, are described by the Lorentz group, or more generally the Poincaré group.'] \n",
      " -------------------------\n",
      "FINAL ANSWER:\n",
      "A 49\n"
     ]
    }
   ],
   "source": [
    "for i in range (len(predicted),len(Questions)):\n",
    "    predict = answer_mcq_graph(Questions[i], Choices[i])\n",
    "    if predict != Answers[i]:\n",
    "        predicted.append(re.search(r'\\b[A-E]\\b', predict).group())\n",
    "        print(\"FINAL ANSWER:\")\n",
    "        print(predict, i)\n",
    "    else:\n",
    "        predicted.append(\"_\")\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db252764",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file['predicted'] = predicted\n",
    "train_file.to_csv(\"Predicted_by_AgenticAI.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
